use anyhow::Result;

use approx::assert_abs_diff_eq;
use ndarray::Array;
use ndarray_glm::{Linear, Logistic, ModelBuilder};
mod common;
use common::{array_from_csv, load_linear_weights_data, y_x_off_from_csv};

#[test]
fn logistic_weights() -> Result<()> {
    let (y, x, wts) = y_x_off_from_csv::<bool, f32, 2>("tests/data/log_weights.csv")?;
    let model = ModelBuilder::<Logistic>::data(&y, &x)
        .var_weights(wts.clone())
        .build()?;
    let fit = model.fit()?;

    let eps: f32 = 1e-4;

    let n_par = fit.result.len();
    let n_obs = y.len();

    let r_result = array_from_csv::<f32>("tests/R/log_weights/coefficients.csv")?;
    // NOTE: R result only seems good to a few decimal points
    assert_abs_diff_eq!(&fit.result, &r_result, epsilon = eps);
    assert!(
        fit.lr_test_against(&r_result) >= 0.,
        "make sure our fit is at least as good as R's"
    );

    // check parameter covariance function
    let r_flat_cov = array_from_csv::<f32>("tests/R/log_weights/covariance.csv")?;
    let r_cov = Array::from_shape_vec((n_par, n_par), r_flat_cov.into_raw_vec_and_offset().0)?;
    assert_abs_diff_eq!(fit.covariance()?, r_cov, epsilon = eps);

    // total deviance uses the weights
    let r_dev = array_from_csv::<f32>("tests/R/log_weights/deviance.csv")?[0];
    assert_abs_diff_eq!(fit.deviance(), r_dev, epsilon = 0.1 * eps);

    let r_null_dev = array_from_csv::<f32>("tests/R/log_weights/null_dev.csv")?[0];
    assert_abs_diff_eq!(fit.lr_test(), r_null_dev - r_dev, epsilon = 0.1 * eps);

    // Residuals should be orthogonal to the hat matrix
    // Our convention of the hat matrix is orthogonal to the response residuals
    let hat_mat = fit.hat()?;
    assert_abs_diff_eq!(
        hat_mat.dot(&fit.resid_resp()),
        Array::zeros(y.len()),
        epsilon = 0.1 * eps,
    );

    let hat = fit.leverage()?;
    let r_hat = array_from_csv::<f32>("tests/R/log_weights/hat.csv")?;
    assert_abs_diff_eq!(hat, r_hat, epsilon = eps);

    let r_resid_pear = array_from_csv::<f32>("tests/R/log_weights/pearson_resid.csv")?;
    assert_abs_diff_eq!(fit.resid_pear(), r_resid_pear, epsilon = eps);

    // Again we have a discrepancy between frequency and variance weights
    let r_resid_dev = array_from_csv::<f32>("tests/R/log_weights/dev_resid.csv")?;
    assert_abs_diff_eq!(fit.resid_dev(), r_resid_dev, epsilon = eps);

    // studentized residuals use the leverage, which depends on the weights
    let r_stand_resid_pear =
        array_from_csv::<f32>("tests/R/log_weights/standard_pearson_resid.csv")?;
    let r_stand_resid_dev =
        array_from_csv::<f32>("tests/R/log_weights/standard_deviance_resid.csv")?;
    assert_abs_diff_eq!(fit.resid_pear_std()?, r_stand_resid_pear, epsilon = eps);
    assert_abs_diff_eq!(fit.resid_dev_std()?, r_stand_resid_dev, epsilon = eps);
    // R's rstudent.glm() doesn't seem to be as precise as our approximation. In this example, most
    // values are close but at least one is off by ~20%. Just skip this one here.
    // let r_stud_resid = array_from_csv::<f32>("tests/R/log_weights/student_resid.csv")?;
    // assert_abs_diff_eq!(fit.resid_student()?, r_stud_resid, epsilon = eps);

    // // These match in the unweighted case, but not the weighted one.
    // The difference between weighted/unweighted is the odd part.
    let r_aic = array_from_csv::<f32>("tests/R/log_weights/aic.csv")?[0];
    // With the sum of the log weights, these are pretty close, but not to FPE.
    assert_abs_diff_eq!(fit.aic(), r_aic, epsilon = 0.2);
    // let r_bic = array_from_csv::<f32>("tests/R/log_weights/bic.csv")?[0];
    // assert_eq!(fit.bic(), r_bic);

    // check the leave-one-out influence coefficients
    // Probably only accurate to within O(1/n_obs).
    // R's method is inexact as well, and it does not match ours, even when unweighted.
    // We'll check with a large epsilon, but the difference may be worth investigating further.
    let r_loo = array_from_csv::<f32>("tests/R/log_weights/loo_coef.csv")?;
    let r_loo = Array::from_shape_vec((n_obs, n_par), r_loo.into_raw_vec_and_offset().0)?;

    let loo_exact = fit.loo_exact()?;
    let infl_exact = &fit.result - &loo_exact;
    let infl_loo = fit.infl_coef()?;

    // The influence results from R seem much worse than ours. Check that the errors are usually
    // lower, component-wise.
    let r_diff = infl_exact.clone() - r_loo.clone();
    let loo_diff = infl_exact.clone() - infl_loo.clone();
    let all_better = loo_diff.mapv(|x| x.abs()) - r_diff.mapv(|x| x.abs());
    // In this case, one of the entries is positive.
    let num_positive = all_better.into_iter().filter(|&x| x > 0.).count();
    dbg!(num_positive);
    assert!(num_positive < 5, "Allowing only four worse elements");
    // R's coefficients seem to be further away from exact than our own. In fact, they seem quite
    // far off.
    // assert_abs_diff_eq!(infl_exact, r_loo, epsilon = eps);
    // let infl_diff = fit.infl_coef()? - &r_loo;
    // This passes instead, but this epsilon is rather huge:
    // assert_abs_diff_eq!(infl_loo, r_loo, epsilon = 0.5);
    // This is close-ish, but not expected to be exact.
    // assert_abs_diff_eq!(infl_loo, infl_exact, epsilon = eps);

    Ok(())
}

// --- Linear model weight test suite ---

/// Load R reference data from a scenario directory and validate all quantities that are
/// comparable between R's glm() and our library. For scenarios without frequency weights,
/// all per-observation quantities can be compared. For freq-weighted scenarios, only
/// aggregates and response/working residuals are compared (leverage semantics differ).
/// NOTE: This method and testing scheme was generated with claude code.
fn check_linear_scenario(
    fit: &ndarray_glm::Fit<Linear, f64>,
    dir: &str,
    eps: f64,
    has_var_weights: bool,
    has_freq_weights: bool,
) -> Result<()> {
    let r = |name: &str| array_from_csv::<f64>(&format!("tests/R/linear_weights/{dir}/{name}.csv"));

    let n_par = fit.result.len();
    let n_obs = 25usize; // original observations

    // --- Coefficients ---
    let r_coef = r("coefficients")?;
    assert_abs_diff_eq!(&fit.result, &r_coef, epsilon = eps);

    // --- Deviance ---
    let r_dev = r("deviance")?[0];
    assert_abs_diff_eq!(fit.deviance(), r_dev, epsilon = eps);

    // --- Null deviance (via lr_test) ---
    let r_null_dev = r("null_deviance")?[0];
    let our_null_dev = fit.lr_test() + r_dev;
    assert_abs_diff_eq!(our_null_dev, r_null_dev, epsilon = 10. * eps);

    // --- Dispersion ---
    // Our library uses an effective sample size correction for variance weights:
    //   dispersion = deviance / ((1 - p/n_eff) * sum_weights)
    // R uses: dispersion = deviance / (n - p)
    // These match when there are no variance weights.
    if !has_var_weights {
        let r_disp = r("dispersion")?[0];
        assert_abs_diff_eq!(fit.dispersion(), r_disp, epsilon = eps);

        // Covariance and Wald Z depend on dispersion, so only check when dispersion matches
        let r_cov_flat = r("covariance")?;
        let r_cov = Array::from_shape_vec((n_par, n_par), r_cov_flat.into_raw_vec_and_offset().0)?;
        assert_abs_diff_eq!(fit.covariance()?, r_cov, epsilon = eps);

        let r_wald_z = r("wald_z")?;
        assert_abs_diff_eq!(fit.wald_z()?, r_wald_z, epsilon = 10. * eps);
    }

    // --- AIC / BIC ---
    // For gaussian, R's AIC includes the full log-likelihood (with normalizing constant
    // -n/2 * log(2*pi*sigma^2)) and counts sigma^2 as a parameter. Our library uses
    // deviance + 2*p, which differs by a data-dependent additive constant.
    // Both are valid (AIC is only meaningful for differences), but values don't match.
    // We skip direct AIC/BIC comparison for the linear family.

    // --- Residuals: response ---
    let r_resid_resp = r("resid_resp")?;
    assert_abs_diff_eq!(fit.resid_resp(), r_resid_resp, epsilon = eps);

    // --- Residuals: working ---
    // For gaussian with identity link, working = response
    let r_resid_work = r("resid_work")?;
    assert_abs_diff_eq!(fit.resid_work(), r_resid_work, epsilon = eps);

    if !has_freq_weights {
        // Per-observation quantities that depend on leverage (which has different semantics
        // under frequency weights due to row duplication in R vs. freq weighting in our library)

        // --- Pearson residuals ---
        let r_resid_pear = r("resid_pear")?;
        assert_abs_diff_eq!(fit.resid_pear(), r_resid_pear, epsilon = eps);

        // --- Deviance residuals ---
        let r_resid_dev = r("resid_dev")?;
        assert_abs_diff_eq!(fit.resid_dev(), r_resid_dev, epsilon = eps);

        // --- Leverage ---
        let r_leverage = r("leverage")?;
        assert_abs_diff_eq!(fit.leverage()?, r_leverage, epsilon = eps);

        if !has_var_weights {
            // Standardized residuals depend on dispersion, which differs with var weights

            // --- Standardized Pearson residuals ---
            let r_resid_pear_std = r("resid_pear_std")?;
            assert_abs_diff_eq!(fit.resid_pear_std()?, r_resid_pear_std, epsilon = eps);

            // --- Standardized deviance residuals ---
            let r_resid_dev_std = r("resid_dev_std")?;
            assert_abs_diff_eq!(fit.resid_dev_std()?, r_resid_dev_std, epsilon = eps);

            // --- Studentized residuals ---
            // R's rstudent.glm() uses a different one-step approximation than ours.
            // Our method is exact for linear models (verified in fit.rs::residuals_linear
            // which compares against actual LOO refits). R's approximation can differ
            // significantly (>10% relative error on some observations), so we skip this
            // comparison here.
            let _r_resid_student = r("resid_student")?;
            let _our_resid_student = fit.resid_student()?;

            // --- Cook's distance ---
            let r_cooks = r("cooks")?;
            assert_abs_diff_eq!(fit.cooks()?, r_cooks, epsilon = eps);
        }

        // --- LOO influence coefficients ---
        let r_loo_flat = r("loo_coef")?;
        let r_loo = Array::from_shape_vec((n_obs, n_par), r_loo_flat.into_raw_vec_and_offset().0)?;
        // R's influence() returns the contribution to subtract from coefficients.
        // Our infl_coef() returns the same thing.
        let infl = fit.infl_coef()?;
        assert_abs_diff_eq!(infl, r_loo, epsilon = 10. * eps);

        // Also check exact LOO against the one-step approximation
        let loo_exact = fit.loo_exact()?;
        let infl_exact = &fit.result - &loo_exact;
        // For linear models, the one-step approximation should be exact
        assert_abs_diff_eq!(infl, infl_exact, epsilon = eps);
    }

    // --- P-values (stats feature) ---
    #[cfg(feature = "stats")]
    {
        // pvalue_lr_test: chi-squared omnibus test â€” valid under all weight configurations.
        let r_lr_p = r("pvalue_lr_test")?[0];
        assert_abs_diff_eq!(fit.pvalue_lr_test(), r_lr_p, epsilon = eps);

        // pvalue_wald and pvalue_exact both use the dispersion estimate in the denominator,
        // so they only match R when our dispersion agrees with R's (i.e. no variance weights).
        if !has_var_weights {
            let r_wald_p = r("pvalue_wald")?;
            assert_abs_diff_eq!(fit.pvalue_wald()?, r_wald_p, epsilon = eps);

            // pvalue_exact: drop-one F-test (intercept via anova, predictors via drop1 in R).
            let r_exact_p = r("pvalue_exact")?;
            assert_abs_diff_eq!(fit.pvalue_exact()?, r_exact_p, epsilon = 10. * eps);
        }
    }

    Ok(())
}

#[test]
fn linear_no_weights() -> Result<()> {
    let (y, x, _var_wt, _freq_wt) = load_linear_weights_data()?;
    let model = ModelBuilder::<Linear>::data(&y, &x).build()?;
    let fit = model.fit()?;
    check_linear_scenario(&fit, "none", 1e-10, false, false)
}

#[test]
fn linear_var_weights() -> Result<()> {
    let (y, x, var_wt, _freq_wt) = load_linear_weights_data()?;
    let model = ModelBuilder::<Linear>::data(&y, &x)
        .var_weights(var_wt)
        .build()?;
    let fit = model.fit()?;
    check_linear_scenario(&fit, "var", 1e-10, true, false)
}

#[test]
fn linear_freq_weights() -> Result<()> {
    let (y, x, _var_wt, freq_wt) = load_linear_weights_data()?;
    let model = ModelBuilder::<Linear>::data(&y, &x)
        .freq_weights(freq_wt)
        .build()?;
    let fit = model.fit()?;
    check_linear_scenario(&fit, "freq", 1e-10, false, true)
}

#[test]
fn linear_both_weights() -> Result<()> {
    let (y, x, var_wt, freq_wt) = load_linear_weights_data()?;
    let model = ModelBuilder::<Linear>::data(&y, &x)
        .var_weights(var_wt)
        .freq_weights(freq_wt)
        .build()?;
    let fit = model.fit()?;
    check_linear_scenario(&fit, "both", 1e-10, true, true)
}

#[test]
fn linear_ridge() -> Result<()> {
    let (y, x, _var_wt, _freq_wt) = load_linear_weights_data()?;
    let model = ModelBuilder::<Linear>::data(&y, &x).build()?;
    let fit_unreg = model.fit()?;

    let model = ModelBuilder::<Linear>::data(&y, &x).build()?;
    let fit = model.fit_options().l2_reg(0.1).fit()?;

    // Ridge should converge
    assert!(fit.n_iter <= 32, "Ridge should converge");

    // Ridge likelihood should be >= unregularized when evaluated with the regularizer
    // (the regularized fit maximizes the penalized likelihood)
    assert!(
        fit.lr_test_against(&fit_unreg.result) >= 0.,
        "Ridge fit should be at least as good under its own penalized likelihood"
    );

    // Coefficients should be shrunk toward zero compared to OLS
    let ridge_norm: f64 = fit.result.mapv(|x| x * x).sum();
    let ols_norm: f64 = fit_unreg.result.mapv(|x| x * x).sum();
    assert!(
        ridge_norm < ols_norm,
        "Ridge coefficients should have smaller L2 norm"
    );

    Ok(())
}

#[test]
fn linear_ridge_var_weights() -> Result<()> {
    let (y, x, var_wt, _freq_wt) = load_linear_weights_data()?;
    let model = ModelBuilder::<Linear>::data(&y, &x)
        .var_weights(var_wt.clone())
        .build()?;
    let fit_unreg = model.fit()?;

    let model = ModelBuilder::<Linear>::data(&y, &x)
        .var_weights(var_wt)
        .build()?;
    let fit = model.fit_options().l2_reg(0.1).fit()?;

    assert!(fit.n_iter <= 32, "Ridge with var weights should converge");

    let ridge_norm: f64 = fit.result.mapv(|x| x * x).sum();
    let ols_norm: f64 = fit_unreg.result.mapv(|x| x * x).sum();
    assert!(
        ridge_norm < ols_norm,
        "Ridge coefficients should have smaller L2 norm"
    );

    Ok(())
}

// --- Linear model with offset and no intercept ---
// These test the null model code path where linear_offset is Some and use_intercept is false.

/// Compute the offset vector matching the R script: off = 0.3 * x1
fn offset_from_x(x: &ndarray::Array2<f64>) -> ndarray::Array1<f64> {
    x.column(0).mapv(|v| 0.3 * v)
}

#[test]
fn linear_offset_noint_no_weights() -> Result<()> {
    let (y, x, _var_wt, _freq_wt) = load_linear_weights_data()?;
    let off = offset_from_x(&x);
    // Fit y ~ x2 + x3 - 1 with offset = 0.3*x1
    let x_sub = x.slice(ndarray::s![.., 1..]).to_owned();
    let model = ModelBuilder::<Linear>::data(&y, &x_sub)
        .linear_offset(off)
        .no_constant()
        .build()?;
    let fit = model.fit()?;
    check_linear_scenario(&fit, "off_none", 1e-10, false, false)
}

#[test]
fn linear_offset_noint_var_weights() -> Result<()> {
    let (y, x, var_wt, _freq_wt) = load_linear_weights_data()?;
    let off = offset_from_x(&x);
    let x_sub = x.slice(ndarray::s![.., 1..]).to_owned();
    let model = ModelBuilder::<Linear>::data(&y, &x_sub)
        .linear_offset(off)
        .no_constant()
        .var_weights(var_wt)
        .build()?;
    let fit = model.fit()?;
    check_linear_scenario(&fit, "off_var", 1e-10, true, false)
}

#[test]
fn linear_offset_noint_freq_weights() -> Result<()> {
    let (y, x, _var_wt, freq_wt) = load_linear_weights_data()?;
    let off = offset_from_x(&x);
    let x_sub = x.slice(ndarray::s![.., 1..]).to_owned();
    let model = ModelBuilder::<Linear>::data(&y, &x_sub)
        .linear_offset(off)
        .no_constant()
        .freq_weights(freq_wt)
        .build()?;
    let fit = model.fit()?;
    check_linear_scenario(&fit, "off_freq", 1e-10, false, true)
}

#[test]
fn linear_offset_noint_both_weights() -> Result<()> {
    let (y, x, var_wt, freq_wt) = load_linear_weights_data()?;
    let off = offset_from_x(&x);
    let x_sub = x.slice(ndarray::s![.., 1..]).to_owned();
    let model = ModelBuilder::<Linear>::data(&y, &x_sub)
        .linear_offset(off)
        .no_constant()
        .var_weights(var_wt)
        .freq_weights(freq_wt)
        .build()?;
    let fit = model.fit()?;
    check_linear_scenario(&fit, "off_both", 1e-10, true, true)
}

#[test]
fn linear_lasso() -> Result<()> {
    let (y, x, _var_wt, _freq_wt) = load_linear_weights_data()?;
    let model = ModelBuilder::<Linear>::data(&y, &x).build()?;
    let fit_unreg = model.fit()?;

    let model = ModelBuilder::<Linear>::data(&y, &x).build()?;
    let fit = model.fit_options().l1_reg(0.1).fit()?;

    assert!(fit.n_iter <= 64, "Lasso should converge");

    // L1 should shrink coefficients
    let lasso_norm: f64 = fit.result.iter().map(|x| x.abs()).sum();
    let ols_norm: f64 = fit_unreg.result.iter().map(|x| x.abs()).sum();
    assert!(
        lasso_norm < ols_norm,
        "Lasso coefficients should have smaller L1 norm"
    );

    Ok(())
}

#[test]
fn linear_elastic_net() -> Result<()> {
    let (y, x, _var_wt, _freq_wt) = load_linear_weights_data()?;
    let model = ModelBuilder::<Linear>::data(&y, &x).build()?;
    let fit_unreg = model.fit()?;

    let model = ModelBuilder::<Linear>::data(&y, &x).build()?;
    let fit = model.fit_options().l1_reg(0.05).l2_reg(0.05).fit()?;

    assert!(fit.n_iter <= 64, "Elastic net should converge");

    // Should shrink both norms
    let en_l1: f64 = fit.result.iter().map(|x| x.abs()).sum();
    let ols_l1: f64 = fit_unreg.result.iter().map(|x| x.abs()).sum();
    assert!(
        en_l1 < ols_l1,
        "Elastic net coefficients should have smaller L1 norm"
    );

    let en_l2: f64 = fit.result.mapv(|x| x * x).sum();
    let ols_l2: f64 = fit_unreg.result.mapv(|x| x * x).sum();
    assert!(
        en_l2 < ols_l2,
        "Elastic net coefficients should have smaller L2 norm"
    );

    Ok(())
}

#[test]
fn counting_weights() -> Result<()> {
    // This actually gets at the distinction between variance weights and frequency weights.
    // The parameter results should be the same, but the covariance and other statistics will
    // distinguish.

    use ndarray::array;
    let x_duped = array![
        [-4.3, 0.2],
        [2.3, 0.4],
        [2.3, 0.4],
        [-1.2, -0.1],
        [2.3, 0.4],
        [-4.3, 0.2],
        [0.5, -0.5],
    ];
    let y_duped = array![0.5, 1.2, 1.2, 0.3, 1.2, 0.5, 0.8];
    let x_red = array![[-4.3, 0.2], [2.3, 0.4], [-1.2, -0.1], [0.5, -0.5]];
    let y_red = array![0.5, 1.2, 0.3, 0.8];
    let freqs_red = array![2, 3, 1, 1];

    let model_duped = ModelBuilder::<Linear>::data(&y_duped, &x_duped).build()?;
    let fit_duped = model_duped.fit()?;

    let model_red = ModelBuilder::<Linear>::data(&y_red, &x_red)
        .freq_weights(freqs_red)
        .build()?;
    let fit_red = model_red.fit()?;

    // let eps = f32::EPSILON as f64;
    let eps = 64.0 * f64::EPSILON;

    assert_abs_diff_eq!(fit_duped.result, fit_red.result, epsilon = eps);
    assert_abs_diff_eq!(
        fit_duped.covariance()?,
        fit_red.covariance()?,
        epsilon = eps
    );

    Ok(())
}
